{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_images(images, cols = 1, titles = None):\n",
    "#     \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "#     Parameters\n",
    "#     ---------\n",
    "#     images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "#     cols (Default = 1): Number of columns in figure (number of rows is \n",
    "#                         set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "#     titles: List of titles corresponding to each image. Must have\n",
    "#             the same length as titles.\n",
    "#     \"\"\"\n",
    "#     assert((titles is None)or (len(images) == len(titles)))\n",
    "#     n_images = len(images)\n",
    "#     if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "#     fig = plt.figure()\n",
    "#     for n, (image, title) in enumerate(zip(images, titles)):\n",
    "#         a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "#         if image.ndim == 2:\n",
    "#             plt.gray()\n",
    "#         plt.imshow(image)\n",
    "#         a.set_title(title)\n",
    "#     fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "#     plt.show()\n",
    "\n",
    "# images = []\n",
    "# for i in range(5):\n",
    "#     images.append(second_layer_weights[:,:, 0, i]/255.0)\n",
    "# show_images(images, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    def __init__(self, data, validation_size=10000, learning_rate=1e-4, dropout=0.5, batch_size=50, num_epochs=20000):\n",
    "        self.validation_size = validation_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.construct()\n",
    "        self.validation_accuracy = []\n",
    "        self.testing_accuracy = []\n",
    "        self.losses = []\n",
    "        self.i  = 0\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def Stats(self):\n",
    "        return self.losses, self.testing_accuracy, self.validation_accuracy\n",
    "    \n",
    "    def nextBatch(self):\n",
    "        indices = self.data[\"train\"][\"images\"].shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        i = self.i\n",
    "        if i + batch_size > indices:\n",
    "            batch_size = indices - i\n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_train = self.data[\"train\"][\"images\"][i:i+self.batch_size, :]\n",
    "        batch_labels = self.data[\"train\"][\"labels\"][i:i+self.batch_size, :]\n",
    "        \n",
    "        i += batch_size\n",
    "        if i >= indices:\n",
    "            self.data[\"train\"][\"images\"], self.data[\"train\"][\"labels\"] = shuffle(self.data[\"train\"][\"images\"], self.data[\"train\"][\"labels\"])\n",
    "    \n",
    "        i %= indices\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.i = i\n",
    "        return batch_train, batch_labels \n",
    "        \n",
    "    def construct(self):\n",
    "        \"\"\"\n",
    "            Graph Setup\n",
    "        \"\"\"\n",
    "        \n",
    "        n_filters = 32\n",
    "        stride_size = 10\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "        def constructConvolutionLayer(x, filter_shape, n_filters=32, channels=1, strides=[1, 1, 1, 1], padding='SAME'):\n",
    "            w = tf.Variable(tf.truncated_normal([filter_shape[0], filter_shape[1], channels, n_filters], stddev= 0.05), name=\"w\")\n",
    "            b = tf.Variable(tf.zeros([n_filters]), name=\"b\")\n",
    "\n",
    "            conv = tf.nn.conv2d(x, w, strides=strides, padding=padding)\n",
    "            return conv + b, w, b\n",
    "\n",
    "        # conv1\n",
    "        with tf.variable_scope(\"conv1\"):\n",
    "            reshaped = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "            conv, w, _ = constructConvolutionLayer(reshaped, [5, 5], 32, 1)\n",
    "        conv1 = tf.nn.relu(conv)\n",
    "\n",
    "        # pooling 1\n",
    "        pooling1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "        # conv2\n",
    "        old_n_filters = n_filters\n",
    "        n_filters = 128\n",
    "        with tf.variable_scope(\"conv2\"):\n",
    "            conv, w, _ = constructConvolutionLayer(pooling1, [5, 5], n_filters, old_n_filters)\n",
    "        conv2 = tf.nn.relu(conv)\n",
    "\n",
    "        # pooling\n",
    "        pooling2 =  tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "        # conv3 \n",
    "        old_n_filters = n_filters\n",
    "        n_filters = 128\n",
    "        with tf.variable_scope(\"conv3\"):\n",
    "            conv, w, _ = constructConvolutionLayer(pooling2, [5, 5], n_filters, old_n_filters)\n",
    "        conv3 = tf.nn.relu(conv)\n",
    "\n",
    "#         # pooling\n",
    "        pooling3 =  tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        \n",
    "#         # conv4\n",
    "        old_n_filters = n_filters\n",
    "        n_filters = 128\n",
    "        with tf.variable_scope(\"conv4\"):\n",
    "            conv, w, _ = constructConvolutionLayer(pooling3, [5, 5], n_filters, old_n_filters)\n",
    "        conv4 = tf.nn.relu(conv)\n",
    "        # pooling\n",
    "        pooling4 =  tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        \n",
    "#         # conv4\n",
    "#         old_n_filters = n_filters\n",
    "#         n_filters = 384\n",
    "#         with tf.variable_scope(\"conv5\"):\n",
    "#             conv, w, _ = constructConvolutionLayer(pooling4, [5, 5], n_filters, old_n_filters)\n",
    "#         conv5 = tf.nn.relu(conv)\n",
    "#         # pooling\n",
    "#         pooling5 =  tf.nn.max_pool(conv5, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        \n",
    "#         # conv4\n",
    "#         old_n_filters = n_filters\n",
    "#         n_filters = 456\n",
    "#         with tf.variable_scope(\"conv6\"):\n",
    "#             conv, w, _ = constructConvolutionLayer(pooling5, [5, 5], n_filters, old_n_filters)\n",
    "#         conv6 = tf.nn.relu(conv)\n",
    "#         # pooling\n",
    "#         pooling6 =  tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        \n",
    "        \n",
    "        # add fully connected layer\n",
    "        fc_i = tf.contrib.layers.flatten(pooling4)\n",
    "        w = tf.Variable(tf.truncated_normal([fc_i.get_shape().as_list()[1], 1024], stddev=0.03))\n",
    "        b = tf.Variable(tf.zeros([1024]))\n",
    "\n",
    "        fc_f = tf.nn.relu(tf.matmul(fc_i, w) + b)\n",
    "\n",
    "        # dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(fc_f, keep_prob)\n",
    "        \n",
    "        # add output layer\n",
    "        w = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.03))\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        \n",
    "        outputs = tf.matmul(h_fc1_drop, w) + b\n",
    "#         outputs = tf.nn.dropout(outputs, keep_prob)\n",
    "        \n",
    "        self.predictions = tf.argmax(tf.nn.softmax(outputs), 1)\n",
    "\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "        # regularization = tf.nn.\n",
    "        loss = tf.losses.softmax_cross_entropy(labels, outputs)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=labels))\n",
    "        train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        self.train_step = train_step\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.x = x\n",
    "        self.labels = labels\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "    def printParameters(self):\n",
    "        print(\"learning rate: {}, batch size: {}, dropout prob: {}, # epochs: {}\".format(self.learning_rate, self.batch_size, self.dropout, self.num_epochs))\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "                Training\n",
    "        \"\"\"\n",
    "        print(\"Starting training.....\")\n",
    "        self.printParameters()\n",
    "        print()\n",
    "        \n",
    "        with tf.Session() as session:\n",
    "            tf.global_variables_initializer().run(session=session)\n",
    "            tf.local_variables_initializer().run(session=session)\n",
    "            \n",
    "            average_loss = 0\n",
    "            start = time.time()\n",
    "            losses = []\n",
    "            testing_accuracy = []\n",
    "            validation_accuracy = []\n",
    "            l = 0\n",
    "            \n",
    "            for i in range(self.num_epochs):\n",
    "                batch_images, batch_labels = self.nextBatch()\n",
    "                _, l = session.run([self.train_step, self.loss], feed_dict={self.x: batch_images, self.labels:batch_labels, self.keep_prob: 0.5})\n",
    "\n",
    "                average_loss += l\n",
    "\n",
    "                if i > 0 and i % 1000 == 0:\n",
    "                    average_loss /= 1000\n",
    "                    validation_loss = session.run(self.accuracy, feed_dict={self.x: self.data[\"validation\"][\"images\"], self.labels: self.data[\"validation\"][\"labels\"], self.keep_prob: self.dropout})\n",
    "                    \n",
    "                    self.losses.append(average_loss)\n",
    "                    self.validation_accuracy.append(validation_loss)\n",
    "                    \n",
    "                    if average_loss < min(self.losses):\n",
    "                        print(\"step: {}, loss: {}*, validation accuracy: {:0.2f}%\"\n",
    "                                          .format(i, average_loss, validation_loss * 100) )\n",
    "                    else:\n",
    "                        print(\"step: {}, loss: {}, validation accuracy: {:0.2f}%\"\n",
    "                                          .format(i, average_loss, validation_loss * 100) )\n",
    "                    \n",
    "                    average_loss = 0\n",
    "                \n",
    "                \n",
    "                losses.append(l)\n",
    "                \n",
    "            end = time.time() - start\n",
    "            \n",
    "            \n",
    "            \n",
    "            results = session.run([self.predictions], feed_dict={self.x: self.data[\"test\"][\"images\"], self.keep_prob: 1.0})\n",
    "            validation_loss = session.run(self.accuracy, feed_dict={self.x: self.data[\"validation\"][\"images\"], self.labels: self.data[\"validation\"][\"labels\"], self.keep_prob: self.dropout})\n",
    "            \n",
    "            print(\"final validation accuracy: {}\".format(validation_loss))\n",
    "            print(\"Training completed in {:.0f}m{:.0f}s\".format(end // 60, end % 60))\n",
    "            print()\n",
    "            \n",
    "        return results, validation_loss, losses\n",
    "        \n",
    "#     def getTestEvaluation(self):\n",
    "#             with tf.Session() as session:\n",
    "#                 tf.global_variables_initializer().run(session=session)\n",
    "#                 tf.local_variables_initializer().run(session=session)\n",
    "\n",
    "#                 results = session.run([self.predictions], feed_dict={self.x: self.data[\"test\"][\"images\"], self.keep_prob: 1.0})\n",
    "            \n",
    "#             return results\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "    Shuffling two arrays in unison\n",
    "    https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "\"\"\"\n",
    "def shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def mnistZip(filepath):\n",
    "    with zipfile.ZipFile(filepath, 'r') as data_zip:\n",
    "        train = data_zip.open('train.csv')\n",
    "        test = data_zip.open('test.csv')\n",
    "\n",
    "        train = pd.read_csv(train, skiprows=0).values\n",
    "        test = pd.read_csv(test, skiprows=0).values\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def formatData(data):\n",
    "    labels = np.zeros(shape=( data.shape[0], 10))\n",
    "    \n",
    "    i = 0\n",
    "    for row in data:\n",
    "        labels[i, row[0]] = 1.0\n",
    "        i += 1\n",
    "    images = data[:, 1:] \n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def loadData():\n",
    "    kaggle_zip = './kaggle mnist/data.zip'\n",
    "\n",
    "    train, test = mnistZip(kaggle_zip)\n",
    "    train_data, train_labels = formatData(train)\n",
    "    \n",
    "\n",
    "    n_data_points = train_data.shape[0]\n",
    "    \n",
    "    validation_size = 2000\n",
    "    data = {}\n",
    "    data[\"train\"] = {}\n",
    "    data[\"train\"][\"images\"] = train_data[:n_data_points - validation_size, :] / 255.0\n",
    "    data[\"train\"][\"labels\"] = train_labels[:n_data_points - validation_size, :]\n",
    "\n",
    "    flipped = np.fliplr(data[\"train\"][\"images\"])\n",
    "    idx = np.random.randint(flipped.shape[0], size=10000)\n",
    "    flipped = flipped[idx, :]\n",
    "    \n",
    "    # add rotated\n",
    "    \n",
    "    data[\"train\"][\"images\"] = np.concatenate((data[\"train\"][\"images\"], flipped))\n",
    "    data[\"train\"][\"labels\"] = np.concatenate((data[\"train\"][\"labels\"], data[\"train\"][\"labels\"][idx, :]))\n",
    "    \n",
    "    data[\"train\"][\"images\"], data[\"train\"][\"labels\"] = shuffle(data[\"train\"][\"images\"], data[\"train\"][\"labels\"])\n",
    "    \n",
    "    data[\"validation\"] = {}\n",
    "    data[\"validation\"][\"images\"] = train_data[n_data_points - validation_size:, :] / 255.0\n",
    "    data[\"validation\"][\"labels\"] = train_labels[n_data_points - validation_size:, :]\n",
    "    \n",
    "\n",
    "    data[\"test\"] = {}\n",
    "    data[\"test\"][\"images\"] = test / 255.0\n",
    "#     data[\"test\"][\"labels\"] = test_labels\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.....\n",
      "learning rate: 0.0004, batch size: 50, dropout prob: 0.5, # epochs: 30000\n",
      "\n",
      "step: 1000, loss: 0.2835795019371435, validation accuracy: 97.50%\n",
      "step: 2000, loss: 0.07607831791229545, validation accuracy: 98.50%\n",
      "step: 3000, loss: 0.04856750481936615, validation accuracy: 97.90%\n",
      "step: 4000, loss: 0.038329718826342285, validation accuracy: 98.30%\n",
      "step: 5000, loss: 0.028408679154948914, validation accuracy: 98.45%\n",
      "step: 6000, loss: 0.024872349584504263, validation accuracy: 98.65%\n",
      "step: 7000, loss: 0.02071105312540749, validation accuracy: 98.70%\n",
      "step: 8000, loss: 0.01606509910063596, validation accuracy: 98.60%\n",
      "step: 9000, loss: 0.01458736346693513, validation accuracy: 99.05%\n",
      "step: 10000, loss: 0.013936494874949404, validation accuracy: 98.20%\n",
      "step: 11000, loss: 0.012654520855456439, validation accuracy: 98.35%\n",
      "step: 12000, loss: 0.009391794633145366, validation accuracy: 98.75%\n",
      "step: 13000, loss: 0.009952345990651451, validation accuracy: 98.95%\n",
      "step: 14000, loss: 0.010756379236996608, validation accuracy: 98.55%\n",
      "step: 15000, loss: 0.009122387017316779, validation accuracy: 98.90%\n",
      "step: 16000, loss: 0.007148578061876975, validation accuracy: 99.00%\n",
      "step: 17000, loss: 0.010027543383445477, validation accuracy: 98.95%\n",
      "step: 18000, loss: 0.007730185528248466, validation accuracy: 98.55%\n",
      "step: 19000, loss: 0.007668583896714509, validation accuracy: 98.85%\n",
      "step: 20000, loss: 0.00850898218286109, validation accuracy: 98.80%\n",
      "step: 21000, loss: 0.005517009199262513, validation accuracy: 99.05%\n",
      "step: 22000, loss: 0.007830886770462274, validation accuracy: 98.70%\n",
      "step: 23000, loss: 0.006560226765484351, validation accuracy: 98.55%\n",
      "step: 24000, loss: 0.0074193879910342845, validation accuracy: 99.15%\n",
      "step: 25000, loss: 0.0023734818451371744, validation accuracy: 99.05%\n",
      "step: 26000, loss: 0.007691887131358418, validation accuracy: 98.65%\n",
      "step: 27000, loss: 0.006137335764897028, validation accuracy: 98.65%\n",
      "step: 28000, loss: 0.004943826062635513, validation accuracy: 99.05%\n",
      "step: 29000, loss: 0.005120806690838402, validation accuracy: 98.70%\n",
      "final validation accuracy: 0.9845000505447388\n",
      "Training completed in 9m2s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = ConvNet(data=data, num_epochs=30000, batch_size=50, learning_rate=4e-4)\n",
    "training_results = c.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nshXTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a59260b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res, v_a, losses = training_results\n",
    "saveResult(res[0], \"final2------------.csv\")\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Testing\n",
    "\"\"\"\n",
    "learning_rates = [1e-4]\n",
    "# batch_sizes = [x for x in range(32, 128, (128-32)//2)]\n",
    "batch_sizes = [100]\n",
    "num_epochs = [20000]\n",
    "dropout = [0.5]\n",
    "\n",
    "results = []\n",
    "for lr in sorted(learning_rates):\n",
    "    for batch_size in sorted(batch_sizes):\n",
    "        for epochs in sorted(num_epochs):\n",
    "            for dr in sorted(dropout):\n",
    "                cnn = ConvNet(data=data, learning_rate=lr, batch_size=batch_size, num_epochs=epochs, dropout=dr)\n",
    "                result, v_a, losses = cnn.train()\n",
    "                \n",
    "                results.append({\"dropout\": dropout, \"num_epochs\": epochs, \"batch_size\": batch_size, \n",
    "                                \"learning_rate\": lr,\"final validation_accuracy\": v_a, \"results\": result,\n",
    "                               \"losses\": losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3e8b31c16be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mkaggle_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msaveResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/output/{}-deep-convnet-2.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# print(results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def plotResults(results):\n",
    "    for result in results:\n",
    "        plt.plot(result[\"losses\"], label=\"learning rate: {}\".format(result[\"learning_rate\"]))\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "def saveResult(result, filename):\n",
    "    indices = [i+1 for i in range(len(result))]\n",
    "    kaggle_submission = pd.DataFrame(data={\"ImageId\": indices, \"Label\": result})\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    with open(filename, \"w\") as f:\n",
    "        kaggle_submission.to_csv(f, index=False)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    saveResult(result[\"results\"][0], \"/output/{}-deep-convnet-2.csv\".format(i))\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4\n",
    "plt.imshow(np.reshape(data[\"test\"][\"images\"][index, :], (28, 28)))\n",
    "plt.show()\n",
    "print((results[0][index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
